{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')  # You can also try 'darkgrid', 'white', 'dark', etc.\n",
    "\n",
    "\n",
    "ds = pd.read_csv(\"kidney_disease.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots a bar graph with features and missing values\n",
    "from matplotlib import style\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "missing = ds.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "(missing/400).plot(kind=\"bar\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the various categorical values  under object features\n",
    "cat_col=[col for col in ds.columns if ds[col].dtype=='object']\n",
    "for col in cat_col:\n",
    "    print('{} has {} values '.format(col,ds[col].unique()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#THis Creates uniformity in the data under the categorical features by unifying ambiguous categorical feature values\n",
    "ds.replace({'\\t?': np.nan, '\\tyes': 'yes', '\\tno': 'no', '\\t43': '43', ' yes': 'yes', 'ckd\\t': 'ckd'}, inplace=True)\n",
    "\n",
    "# Convert numeric columns to appropriate types after cleaning\n",
    "\n",
    "ds['pcv'] = pd.to_numeric(ds['pcv'], errors='coerce')\n",
    "\n",
    "# wc\n",
    "ds['wc'] = pd.to_numeric(ds['wc'], errors='coerce')\n",
    "\n",
    "# rc\n",
    "ds['rc'] = pd.to_numeric(ds['rc'], errors='coerce')\n",
    "\n",
    "#filling the missing values using mode of the categorical features\n",
    "# For categorical columns with 'normal', 'abnormal', 'notpresent', 'present', etc.\n",
    "for col in ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'classification']:\n",
    "    ds[col].fillna(ds[col].mode()[0], inplace=True)  # fill missing values with mode\n",
    "\n",
    "# Check and verify the data\n",
    "# print(ds.head())\n",
    "ds['classification']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes ckd : 0, not ckd:1\n",
    "ds['classification'] = (ds['classification'] != 'ckd').astype(int)\n",
    "ds['classification']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel density function gives the kernel density curve for the any numerical feature with the ckd or notckd classification or 0 or 1,\n",
    "def kde(col):\n",
    "    grid = sns.FacetGrid(ds, hue=\"classification\", height=6, aspect=2)\n",
    "\n",
    "    grid.map(sns.kdeplot, col)\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffrentiation of categorical and numerical features\n",
    "num_col=[col for col in ds.columns if ds[col].dtype!='object']\n",
    "cat_col=[col for col in ds.columns if ds[col].dtype=='object']\n",
    "\n",
    "\n",
    "print(num_col)\n",
    "print(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots KDE of hemoglobin levels with classification of ckd or notckd\n",
    "kde('hemo')\n",
    "#we can see the subjects with lower hemo levels have ckd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_col:\n",
    "    kde(i)\n",
    "#plots kernel density estimate graph for all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.isnull().sum()/ds.shape[0]*100.00).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values with mean of the numerical features\n",
    "def fill_mean(ftr):\n",
    "    sample = ds[ftr].dropna().sample(ds[ftr].isnull().sum())\n",
    "    sample.index= ds[ds[ftr].isnull()].index\n",
    "    ds.loc[ds[ftr].isnull(),ftr]=sample\n",
    "\n",
    "#filling missing values with mode of the categorical features\n",
    "def fill_mode(ftr):\n",
    "    mode = ds[ftr].mode()[0]\n",
    "    ds[ftr]=ds[ftr].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values with mode of the categorical features\n",
    "for cat in cat_col:\n",
    "    fill_mode(cat)\n",
    "\n",
    "\n",
    "#filling missing values with mean of the numerical features\n",
    "for num in num_col:\n",
    "    fill_mean(num)\n",
    "\n",
    "\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= LabelEncoder()\n",
    "\n",
    "# encode categorical features\n",
    "for col in cat_col:\n",
    "    ds[col]=encoder.fit_transform(ds[col])\n",
    "\n",
    "independent_col = [col for col in ds.columns if col != \"classification\"]\n",
    "dependent_col = \"classification\"\n",
    "\n",
    "X = ds[independent_col]\n",
    "y = ds[dependent_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "rf_precision = precision_score(y_test, y_pred)\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "rf_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Write metrics to file\n",
    "with open('random_forest_metrics.txt', 'w') as f:\n",
    "    f.write(\"Random Forest Metrics:\\n\")\n",
    "    f.write(f\"Accuracy: {rf_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {rf_precision:.4f}\\n\") \n",
    "    f.write(f\"Recall: {rf_recall:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {rf_f1:.4f}\\n\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report\", cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first tree from random forest to visualize\n",
    "tree = rf.estimators_[0]\n",
    "\n",
    "class_names = [\"ckd\", \"notckd\"]\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(tree, feature_names=independent_col, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Separate data by class\n",
    "def separate_by_class(X, y):\n",
    "    classes = np.unique(y)\n",
    "    separated = {c: X[y == c].values for c in classes}\n",
    "    return separated\n",
    "\n",
    "# 2. Calculate Mean and Variance for each feature in each class\n",
    "def calculate_mean_variance(separated_by_class):\n",
    "    mean_variance = {}\n",
    "    for class_value, instances in separated_by_class.items():\n",
    "        # Add small epsilon to variance to avoid division by zero\n",
    "        mean = np.mean(instances, axis=0)\n",
    "        variance = np.var(instances, axis=0) + 1e-9\n",
    "        mean_variance[class_value] = (mean, variance)\n",
    "    return mean_variance\n",
    "\n",
    "# 3. Calculate Prior Probabilities for each class\n",
    "def calculate_prior(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    priors = {c: count / len(y) for c, count in zip(classes, counts)}\n",
    "    return priors\n",
    "\n",
    "# 4. Gaussian Probability Density Function\n",
    "def gaussian_probability(x, mean, variance):\n",
    "    exponent = np.exp(-((x - mean) ** 2) / (2 * variance))\n",
    "    return (1 / np.sqrt(2 * np.pi * variance)) * exponent\n",
    "\n",
    "# 5. Calculate Class Probabilities\n",
    "def calculate_class_probabilities(input_data, mean_variance, priors):\n",
    "    probabilities = {}\n",
    "    for class_value, (mean, variance) in mean_variance.items():\n",
    "        # Start with the prior probability\n",
    "        probabilities[class_value] = np.log(priors[class_value])\n",
    "        # Add log probabilities to avoid numerical underflow\n",
    "        for i in range(len(mean)):\n",
    "            probabilities[class_value] += np.log(gaussian_probability(input_data[i], mean[i], variance[i]))\n",
    "    return probabilities\n",
    "\n",
    "# 6. Prediction\n",
    "def predict(input_data, mean_variance, priors):\n",
    "    probabilities = calculate_class_probabilities(input_data, mean_variance, priors)\n",
    "    # Return the class with the highest probability\n",
    "    return max(probabilities, key=probabilities.get)\n",
    "\n",
    "# 7. Naive Bayes Training Function\n",
    "def train_naive_bayes(X, y):\n",
    "    separated_by_class = separate_by_class(X, y)\n",
    "    mean_variance = calculate_mean_variance(separated_by_class)\n",
    "    priors = calculate_prior(y)\n",
    "    return mean_variance, priors\n",
    "\n",
    "# 8. Predict Multiple Samples\n",
    "def predict_multiple(X, mean_variance, priors):\n",
    "    predictions = [predict(x, mean_variance, priors) for x in X.values]\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Train the model\n",
    "mean_variance, priors = train_naive_bayes(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_multiple(X_test, mean_variance, priors)\n",
    "\n",
    "# Calculate metrics\n",
    "nb_accuracy = np.mean(predictions == y_test)\n",
    "nb_precision = precision_score(y_test, predictions)\n",
    "nb_recall = recall_score(y_test, predictions)\n",
    "nb_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Write metrics to file\n",
    "with open('naive_bayes_metrics.txt', 'w') as f:\n",
    "    f.write(f\"Naive Bayes Metrics:\\n\")\n",
    "    f.write(f\"Accuracy: {nb_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {nb_precision:.4f}\\n\") \n",
    "    f.write(f\"Recall: {nb_recall:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {nb_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Implementation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 2. Cost Function\n",
    "def compute_cost(X, y, weights):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, weights))\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1-y) * np.log(1-h))\n",
    "    return cost\n",
    "\n",
    "# 3. Gradient Descent\n",
    "def gradient_descent(X, y, weights, learning_rate, n_iterations):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        h = sigmoid(np.dot(X, weights))\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        weights = weights - learning_rate * gradient\n",
    "        cost_history.append(compute_cost(X, y, weights))\n",
    "    \n",
    "    return weights, cost_history\n",
    "\n",
    "# 4. Predict Function\n",
    "def predict_logistic(X, weights, threshold=0.5):\n",
    "    probabilities = sigmoid(np.dot(X, weights))\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# 5. Train Logistic Regression\n",
    "def train_logistic_regression(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    # Add bias term\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Train model\n",
    "    weights, cost_history = gradient_descent(X, y, weights, learning_rate, n_iterations)\n",
    "    return weights, cost_history\n",
    "\n",
    "# Train the logistic regression model\n",
    "weights, cost_history = train_logistic_regression(X_train, y_train)\n",
    "\n",
    "# Add bias term to test data\n",
    "X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = predict_logistic(X_test_bias, weights)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_accuracy = np.mean(lr_predictions == y_test)\n",
    "lr_precision = precision_score(y_test, lr_predictions)\n",
    "lr_recall = recall_score(y_test, lr_predictions)\n",
    "lr_f1 = f1_score(y_test, lr_predictions)\n",
    "\n",
    "# Write metrics to file\n",
    "with open('logistic_regression_metrics.txt', 'w') as f:\n",
    "    f.write(\"Logistic Regression Metrics\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    f.write(f\"Accuracy: {lr_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {lr_precision:.4f}\\n\") \n",
    "    f.write(f\"Recall: {lr_recall:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {lr_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics across all models\n",
    "print(\"Comparing Model Performance\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic Regression Metrics\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\") \n",
    "print(f\"F1-Score: {lr_f1:.4f}\")\n",
    "print()\n",
    "\n",
    "# Naive Bayes Metrics\n",
    "print(\"Naive Bayes:\")\n",
    "print(f\"Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"F1-Score: {nb_f1:.4f}\")\n",
    "print()\n",
    "\n",
    "# Random Forest Metrics\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\") \n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall: {rf_recall:.4f}\")\n",
    "print(f\"F1-Score: {rf_f1:.4f}\")\n",
    "\n",
    "# Visualize comparison with bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "lr_scores = [lr_accuracy, lr_precision, lr_recall, lr_f1]\n",
    "nb_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
    "rf_scores = [rf_accuracy, rf_precision, rf_recall, rf_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - width, lr_scores, width, label='Logistic Regression')\n",
    "ax.bar(x, nb_scores, width, label='Naive Bayes')\n",
    "ax.bar(x + width, rf_scores, width, label='Random Forest')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
